% Please download the latest anthology.bib from
%
% http://aclweb.org/anthology/anthology.bib.gz
% Please download the latest anthology.bib from
%
% http://aclweb.org/anthology/anthology.bib.gz
% 1

% 2
@inproceedings{sentenceRE-Lyu,
    title = "Relation Classification with Entity Type Restriction",
    author = "Lyu, Shengfei  and
      Chen, Huanhuan",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2021.findings-acl.34",
    pages = "390--395",
}
% 3
@inproceedings{sentenceRE-Dixit,
    title = "Span-Level Model for Relation Extraction",
    author = "Dixit, Kalpit  and
      Al-Onaizan, Yaser",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/P19-1525",
    pages = "5308--5314",
    abstract = "Relation Extraction is the task of identifying entity mention spans in raw text and then identifying relations between pairs of the entity mentions. Recent approaches for this span-level task have been token-level models which have inherent limitations. They cannot easily define and implement span-level features, cannot model overlapping entity mentions and have cascading errors due to the use of sequential decoding. To address these concerns, we present a model which directly models all possible spans and performs joint entity mention detection and relation extraction. We report a new state-of-the-art performance of 62.83 F1 (prev best was 60.49) on the ACE2005 dataset.",
}
% 4
@inproceedings{zhou2021document,
  title={Document-level relation extraction with adaptive thresholding and localized context pooling},
  author={Zhou, Wenxuan and Huang, Kevin and Ma, Tengyu and Huang, Jing},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={16},
  pages={14612--14620},
  year={2021},
url={https://ojs.aaai.org/index.php/AAAI/article/view/17717/17524}
}
% 5
@article{Zhao2022DocumentlevelRE,
  title={Document-Level Relation Extraction With Context Guided Mention Integration and Inter-Pair Reasoning},
  author={Daojian Zeng and Chao Zhao and Chao Jiang and Jianling Zhu and Jianhua Dai},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2022},
  volume={31},
  pages={3659-3666},
  url={https://api.semanticscholar.org/CorpusID:245906424}
}6
@inproceedings{BERT,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of naacL-HLT},
  volume={1},
  pages={2},
  year={2019},
url={https://aclanthology.org/N19-1423.pdf}
}
% 7

% 8
@inproceedings{GAIN,
    title = "Double Graph Based Reasoning for Document-level Relation Extraction",
    author = "Zeng, Shuang  and
      Xu, Runxin  and
      Chang, Baobao  and
      Li, Lei",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2020.emnlp-main.127",
    pages = "1630--1640",
    abstract = "Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN constructs two graphs, a heterogeneous mention-level graph (MG) and an entity-level graph (EG). The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities. Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at https://github.com/PKUnlp-icler/GAIN.",
}
% 9
@inproceedings{SagDRE,
author = {Wei, Ying and Li, Qi},
title = {SagDRE: Sequence-Aware Graph-Based Document-Level Relation Extraction with Adaptive Margin Loss},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3534678.3539304},
abstract = {Relation extraction (RE) is an important task for many natural language processing applications. Document-level relation extraction task aims to extract the relations within a document and poses many challenges to the RE tasks as it requires reasoning across sentences and handling multiple relations expressed in the same document. Existing state-of-the-art document-level RE models use the graph structure to better connect long-distance correlations. In this work, we propose SagDRE model, which further considers and captures the original sequential information from the text. The proposed model learns sentence-level directional edges to capture the information flow in the document and uses the token-level sequential information to encode the shortest paths from one entity to the other. In addition, we propose an adaptive margin loss to address the long-tailed multi-label problem of document-level RE tasks, where multiple relations can be expressed in a document for an entity pair and there are a few popular relations. The loss function aims to encourage separations between positive and negative classes. The experimental results on datasets from various domains demonstrate the effectiveness of the proposed methods.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2000–2008},
numpages = {9},
keywords = {graph, document-level re, sequence information, relation extraction},
location = {Washington DC, USA},
series = {KDD '22}
}
% 10
@inproceedings{
GCN,
title={Semi-Supervised Classification with Graph Convolutional Networks},
author={Thomas N. Kipf and Max Welling},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=SJU4ayYgl}
}
% 11
@inproceedings{qin-etal-2021-relation,
    title = "Relation Extraction with Word Graphs from N-grams",
    author = "Qin, Han  and
      Tian, Yuanhe  and
      Song, Yan",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2021.emnlp-main.228",
    pages = "2860--2868",
    abstract = "Most recent studies for relation extraction (RE) leverage the dependency tree of the input sentence to incorporate syntax-driven contextual information to improve model performance, with little attention paid to the limitation where high-quality dependency parsers in most cases unavailable, especially for in-domain scenarios. To address this limitation, in this paper, we propose attentive graph convolutional networks (A-GCN) to improve neural RE methods with an unsupervised manner to build the context graph, without relying on the existence of a dependency parser. Specifically, we construct the graph from n-grams extracted from a lexicon built from pointwise mutual information (PMI) and apply attention over the graph. Therefore, different word pairs from the contexts within and across n-grams are weighted in the model and facilitate RE accordingly. Experimental results with further analyses on two English benchmark datasets for RE demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on both datasets.",
}
% 12
@inproceedings{xu-etal-2016-improved,
    title = "Improved relation classification by deep recurrent neural networks with data augmentation",
    author = "Xu, Yan  and
      Jia, Ran  and
      Mou, Lili  and
      Li, Ge  and
      Chen, Yunchuan  and
      Lu, Yangyang  and
      Jin, Zhi",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    pages = "1461--1470",
    abstract = "Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent in comparison with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures (e.g., one-layer convolutional neural networks or recurrent networks). They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks (DRNNs) for relation classification to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluated our DRNNs on the SemEval-2010 Task 8, and achieve an F1-score of 86.1{\%}, outperforming previous state-of-the-art recorded results.",
url={https://arxiv.org/abs/1601.03651}
}
% 13
@inproceedings{DOCRED,
    title = "{D}oc{RED}: A Large-Scale Document-Level Relation Extraction Dataset",
    author = "Yao, Yuan  and
      Ye, Deming  and
      Li, Peng  and
      Han, Xu  and
      Lin, Yankai  and
      Liu, Zhenghao  and
      Liu, Zhiyuan  and
      Huang, Lixin  and
      Zhou, Jie  and
      Sun, Maosong",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/P19-1074",
    pages = "764--777",
    abstract = "Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED.",
}
% 14
@article{li2016biocreative,
  title={BioCreative V CDR task corpus: a resource for chemical disease relation extraction},
  author={Li, Jiao and Sun, Yueping and Johnson, Robin J and Sciaky, Daniela and Wei, Chih-Hsuan and Leaman, Robert and Davis, Allan Peter and Mattingly, Carolyn J and Wiegers, Thomas C and Lu, Zhiyong},
  journal={Database},
  volume={2016},
  year={2016},
  publisher={Oxford Academic},
url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4860626/}
}
% 15
@inproceedings{GDA,
  title={Renet: A deep learning approach for extracting gene-disease associations from literature},
  author={Wu, Ye and Luo, Ruibang and Leung, Henry CM and Ting, Hing-Fung and Lam, Tak-Wah},
  booktitle={Research in Computational Molecular Biology: 23rd Annual International Conference, RECOMB 2019, Washington, DC, USA, May 5-8, 2019, Proceedings 23},
  pages={272--284},
  year={2019},
  organization={Springer},
url={https://link.springer.com/chapter/10.1007/978-3-030-17083-7_17}
}
% 16
@article{pawar2017relation,
  title={Relation extraction: A survey},
  author={Pawar, Sachin and Palshikar, Girish K and Bhattacharyya, Pushpak},
  journal={arXiv preprint arXiv:1712.05191},
  year={2017}
}
% 17
@inproceedings{sahu2019inter,
    title = "Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network",
    author = "Sahu, Sunil Kumar  and
      Christopoulou, Fenia  and
      Miwa, Makoto  and
      Ananiadou, Sophia",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/P19-1423",
    pages = "4309--4316",
    abstract = "Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.",
}
% 18
@inproceedings{y2020-coreferential,
    title = "{C}oreferential {R}easoning {L}earning for {L}anguage {R}epresentation",
    author = "Ye, Deming  and
      Lin, Yankai  and
      Du, Jiaju  and
      Liu, Zhenghao  and
      Li, Peng  and
      Sun, Maosong  and
      Liu, Zhiyuan",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2020.emnlp-main.582",
    pages = "7170--7186",
    abstract = "Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning. However, most existing language representation models cannot explicitly handle coreference, which is essential to the coherent understanding of the whole discourse. To address this issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context. The experimental results show that, compared with existing baseline models, CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/CorefBERT.",
}
% 19
@article{Tang2020HINHI,
  title={HIN: Hierarchical Inference Network for Document-Level Relation Extraction},
  author={Hengzhu Tang and Yanan Cao and Zhenyu Zhang and Jiangxia Cao and Fang Fang and Shi Wang and Pengfei Yin},
  journal={Advances in Knowledge Discovery and Data Mining},
  year={2020},
  volume={12084},
  pages={197 - 209},
url={https://arxiv.org/abs/2003.12754}
}
% 20
@inproceedings{Xu2020DocumentLevelRE,
  title={Document-level relation extraction with reconstruction},
  author={Xu, Wang and Chen, Kehai and Zhao, Tiejun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={16},
  pages={14167--14175},
  year={2021},
url={https://ojs.aaai.org/index.php/AAAI/article/view/17667/17474}
}
% 21
@inproceedings{Dong2022SyntacticML,
  title={Syntactic Multi-view Learning for Open Information Extraction},
  author={Kuicai Dong and Aixin Sun and Jung-jae Kim and Xiaoli Li},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
url= {https://aclanthology.org/2022.emnlp-main.272/},
  year={2022}
}
% 22
@inproceedings{zhang-etal-2017-position,
    title = "Position-aware Attention and Supervised Data Improve Slot Filling",
    author = "Zhang, Yuhao  and
      Zhong, Victor  and
      Chen, Danqi  and
      Angeli, Gabor  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/D17-1004",
    pages = "35--45",
    abstract = "Organized relational knowledge in the form of {``}knowledge graphs{''} is important for many applications. However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly. This paper simultaneously addresses two issues that have held back prior work. We first propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction. Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset obtained via crowdsourcing and targeted towards TAC KBP relations. The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance. When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F1 score increases markedly from 22.2{\%} to 26.7{\%}.",
}
% 23
@inproceedings{jia-etal-2019-document,
    title = "Document-Level N-ary Relation Extraction with Multiscale Representation Learning",
    author = "Jia, Robin  and
      Wong, Cliff  and
      Poon, Hoifung",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/N19-1370",
    pages = "3693--3704",
    abstract = "Most information extraction methods focus on binary relations expressed within single sentences. In high-value domains, however, n-ary relations are of great demand (e.g., drug-gene-mutation interactions in precision oncology). Such relations often involve entity mentions that are far apart in the document, yet existing work on cross-sentence relation extraction is generally confined to small text spans (e.g., three consecutive sentences), which severely limits recall. In this paper, we propose a novel multiscale neural architecture for document-level n-ary relation extraction. Our system combines representations learned over various text spans throughout the document and across the subrelation hierarchy. Widening the system{'}s purview to the entire document maximizes potential recall. Moreover, by integrating weak signals across the document, multiscale modeling increases precision, even in the presence of noisy labels from distant supervision. Experiments on biomedical machine reading show that our approach substantially outperforms previous n-ary relation extraction methods.",
}
% 24
@inproceedings{mou-etal-2016-natural,
    title = "Natural Language Inference by Tree-Based Convolution and Heuristic Matching",
    author = "Mou, Lili  and
      Men, Rui  and
      Li, Ge  and
      Xu, Yan  and
      Zhang, Lu  and
      Yan, Rui  and
      Jin, Zhi",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/P16-2022",
    pages = "130--136",
}
% 25
@inproceedings{miwa-bansal-2016-end,
    title = "End-to-End Relation Extraction using {LSTM}s on Sequences and Tree Structures",
    author = "Miwa, Makoto  and
      Bansal, Mohit",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/P16-1105",
    pages = "1105--1116",
}
% 26
@inproceedings{Gentile1998LinearHL,
 author = {Gentile, Claudio and Warmuth, Manfred K. K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 pages = {},
 publisher = {MIT Press},
 title = {Linear Hinge Loss and Average Margin},
url={http://papers.neurips.cc/paper/1610-linear-hinge-loss-and-average-margin.pdf},
 volume = {11},
 year = {1998}
}
% 27

% 28
@inproceedings{xie2022eider,
  title={Eider: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion},
  author={Xie, Yiqing and Shen, Jiaming and Li, Sha and Mao, Yuning and Han, Jiawei},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={257--268},
  year={2022},
url={https://aclanthology.org/2022.findings-acl.23/}
}
% 29
@inproceedings{nan-etal-2020-reasoning,
    title = "Reasoning with Latent Structure Refinement for Document-Level Relation Extraction",
    author = "Nan, Guoshun  and
      Guo, Zhijiang  and
      Sekulic, Ivan  and
      Lu, Wei",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2020.acl-main.141",
    pages = "1546--1557",
    abstract = "Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.",
}
% 30
@inproceedings{zhou-zhao-2019-head,
    title = "{H}ead-{D}riven {P}hrase {S}tructure {G}rammar Parsing on {P}enn {T}reebank",
    author = "Zhou, Junru  and
      Zhao, Hai",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/P19-1230",
    pages = "2396--2408",
    abstract = "Head-driven phrase structure grammar (HPSG) enjoys a uniform formalism representing rich contextual syntactic and even semantic meanings. This paper makes the first attempt to formulate a simplified HPSG by integrating constituent and dependency formal representations into head-driven phrase structure. Then two parsing algorithms are respectively proposed for two converted tree representations, division span and joint span. As HPSG encodes both constituent and dependency structure information, the proposed HPSG parsers may be regarded as a sort of joint decoder for both types of structures and thus are evaluated in terms of extracted or converted constituent and dependency parsing trees. Our parser achieves new state-of-the-art performance for both parsing tasks on Penn Treebank (PTB) and Chinese Penn Treebank, verifying the effectiveness of joint learning constituent and dependency structures. In details, we report 95.84 F1 of constituent parsing and 97.00{\%} UAS of dependency parsing on PTB.",
}
% 31
@inproceedings{strzyz-etal-2019-sequence,
    title = "Sequence Labeling Parsing by Learning across Representations",
    author = "Strzyz, Michalina  and
      Vilares, David  and
      G{\'o}mez-Rodr{\'\i}guez, Carlos",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/P19-1531",
    pages = "5350--5357",
    abstract = "We use parsing as sequence labeling as a common framework to learn across constituency and dependency syntactic abstractions.To do so, we cast the problem as multitask learning (MTL). First, we show that adding a parsing paradigm as an auxiliary loss consistently improves the performance on the other paradigm. Secondly, we explore an MTL sequence labeling model that parses both representations, at almost no cost in terms of performance and speed. The results across the board show that on average MTL models with auxiliary losses for constituency parsing outperform single-task ones by 1.05 F1 points, and for dependency parsing by 0.62 UAS points.",
}
% 32
@inproceedings{fei-etal-2021-better,
    title = "Better Combine Them Together! Integrating Syntactic Constituency and Dependency Representations for Semantic Role Labeling",
    author = "Fei, Hao  and
      Wu, Shengqiong  and
      Ren, Yafeng  and
      Li, Fei  and
      Ji, Donghong",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2021.findings-acl.49",
    pages = "549--559",
}
% 33
@inproceedings{ma-etal-2023-DREEAM,
    title = "DREEAM: Guiding Attention with Evidence for Improving Document-Level Relation Extraction",
    author = "Ma, Youmi  and
      Wang, An  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    series = {EACL},
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    pages = "(to appear)",
url={https://aclanthology.org/2023.eacl-main.145.pdf}
}

% 35
@inproceedings{bai-etal-2021-syntax,
    title = "Syntax-{BERT}: Improving Pre-trained Transformers with Syntax Trees",
    author = "Bai, Jiangang  and
      Wang, Yujing  and
      Chen, Yiren  and
      Yang, Yaming  and
      Bai, Jing  and
      Yu, Jing  and
      Tong, Yunhai",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2021.eacl-main.262",
    pages = "3011--3020",
    abstract = "Pre-trained language models like BERT achieve superior performances in various NLP tasks without explicit consideration of syntactic information. Meanwhile, syntactic information has been proved to be crucial for the success of NLP applications. However, how to incorporate the syntax trees effectively and efficiently into pre-trained Transformers is still unsettled. In this paper, we address this problem by proposing a novel framework named Syntax-BERT. This framework works in a plug-and-play mode and is applicable to an arbitrary pre-trained checkpoint based on Transformer architecture. Experiments on various datasets of natural language understanding verify the effectiveness of syntax trees and achieve consistent improvement over multiple pre-trained models, including BERT, RoBERTa, and T5.",
}
% 37
@inproceedings{gupta2019neural,
  title={Neural relation extraction within and across sentence boundaries},
  author={Gupta, Pankaj and Rajaram, Subburam and Sch{\"u}tze, Hinrich and Runkler, Thomas},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={6513--6520},
  year={2019}
}
% 38
@inproceedings{duan-etal-2022-just,
    title = "Not Just Plain Text! Fuel Document-Level Relation Extraction with Explicit Syntax Refinement and Subsentence Modeling",
    author = "Duan, Zhichao  and
      Li, Xiuxing  and
      Li, Zhenyu  and
      Wang, Zhuo  and
      Wang, Jianyong",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    pages = "1941--1951",
    abstract = "Document-level relation extraction (DocRE) aims to identify semantic labels among entities within a single document. One major challenge of DocRE is to dig decisive details regarding a specific entity pair from long text. However, in many cases, only a fraction of text carries required information, even in the manually labeled supporting evidence. To better capture and exploit instructive information, we propose a novel expLicit syntAx Refinement and Subsentence mOdeliNg based framework (LARSON). By introducing extra syntactic information, LARSON can model subsentences of arbitrary granularity and efficiently screen instructive ones. Moreover, we incorporate refined syntax into text representations which further improves the performance of LARSON. Experimental results on three benchmark datasets (DocRED, CDR, and GDA) demonstrate that LARSON significantly outperforms existing methods.",
url={https://aclanthology.org/2022.findings-emnlp.140/}
}
% 39
@inproceedings{bunescu-mooney-2005-shortest,
    title = "A Shortest Path Dependency Kernel for Relation Extraction",
    author = "Bunescu, Razvan  and
      Mooney, Raymond",
    booktitle = "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2005",
    address = "Vancouver, British Columbia, Canada",
    publisher = "Association for Computational Linguistics",
    pages = "724--731",
}
%40
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019},
url={https://arxiv.org/abs/1912.01703}
}

@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7}
}

@article{DBLP:journals/corr/GoyalDGNWKTJH17,
  author       = {Priya Goyal and
                  Piotr Doll{\'{a}}r and
                  Ross B. Girshick and
                  Pieter Noordhuis and
                  Lukasz Wesolowski and
                  Aapo Kyrola and
                  Andrew Tulloch and
                  Yangqing Jia and
                  Kaiming He},
  title        = {Accurate, Large Minibatch {SGD:} Training ImageNet in 1 Hour},
  journal      = {CoRR},
  volume       = {abs/1706.02677},
  year         = {2017},
  eprinttype    = {arXiv},
  eprint       = {1706.02677},
  timestamp    = {Mon, 13 Aug 2018 16:49:10 +0200},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kendall2018multi,
  title={Multi-task learning using uncertainty to weigh losses for scene geometry and semantics},
  author={Kendall, Alex and Gal, Yarin and Cipolla, Roberto},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7482--7491},
  year={2018},
    url={https://arxiv.org/abs/1705.07115}
}

@inproceedings{qi2020stanza,
    title = "{S}tanza: A Python Natural Language Processing Toolkit for Many Human Languages",
    author = "Qi, Peng  and
      Zhang, Yuhao  and
      Zhang, Yuhui  and
      Bolton, Jason  and
      Manning, Christopher D.",
    editor = "Celikyilmaz, Asli  and
      Wen, Tsung-Hsien",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-demos.14",
    doi = "10.18653/v1/2020.acl-demos.14",
    pages = "101--108",
    abstract = "We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at \url{https://stanfordnlp.github.io/stanza/}.",
}

@inproceedings{xu-etal-2021-discriminative,
    title = "Discriminative Reasoning for Document-level Relation Extraction",
    author = "Xu, Wang  and
      Chen, Kehai  and
      Zhao, Tiejun",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2021.findings-acl.144",
    pages = "1653--1663",
}

@article{xue2022corefdre,
author = {Xu, Wang and Chen, Kehai and Zhao, Tiejun},
title = {Document-Level Relation Extraction with Path Reasoning},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3572898},
doi = {10.1145/3572898},
abstract = {Document-level relation extraction (DocRE) aims to extract relations among entities across multiple sentences within a document by using reasoning skills (i.e., pattern recognition, logical reasoning, coreference reasoning, etc.) related to the reasoning paths between two entities. However, most of the advanced DocRE models only attend to the feature representations of two entities to determine their relation, and do not consider one complete reasoning path from one entity to another entity, which may hinder the accuracy of relation extraction. To address this issue, this article proposes a novel method to capture this reasoning path from one entity to another entity, thereby better simulating reasoning skills to classify relation between two entities. Furthermore, we introduce an additional attention layer to summarize multiple reasoning paths for further enhancing the performance of the DocRE model. Experimental results on a large-scale document-level dataset show that the proposed approach achieved a significant performance improvement on a strong heterogeneous graph-based baseline.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {104},
numpages = {14},
keywords = {path reasoning, graph neural network, Document-level relation extraction}
}



@inproceedings{liu2023document,
  title={Document-Level Relation Extraction with Cross-sentence Reasoning Graph},
  author={Liu, Hongfei and Kang, Zhao and Zhang, Lizong and Tian, Ling and Hua, Fujun},
  booktitle={Pacific-Asia Conference on Knowledge Discovery and Data Mining},
  pages={316--328},
  year={2023},
  organization={Springer},
url={https://arxiv.org/pdf/2303.03912.pdf}
}

@article{DAI2022107659,
title = {Graph Fusion Network for Text Classification},
journal = {Knowledge-Based Systems},
volume = {236},
pages = {107659},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107659},
author = {Yong Dai and Linjun Shou and Ming Gong and Xiaolin Xia and Zhao Kang and Zenglin Xu and Daxin Jiang},
keywords = {Graph Neural Networks, Text classification, External knowledge, Graph fusion},
}

@inproceedings{delaunay2023comprehensive,
  title={A Survey on Document-Level Relation Extraction: Methods and Applications},
  author={Yifan Zheng and Yikai Guo and Zhizhao Luo and Zengwen Yu and Kunlong Wang and Hong Zhang and Hua Zhao},
  year={2023},
  booktitle={Proceedings of the 3rd International Conference on Internet, Education and Information Technology (IEIT 2023)},
  pages={1061-1071},
  issn={2667-128X},
  isbn={978-94-6463-230-9},
  url={https://doi.org/10.2991/978-94-6463-230-9_128},
  doi={10.2991/978-94-6463-230-9_128},
  publisher={Atlantis Press}
}
@ARTICLE{9098945,
  author={Han, Xiaoyu and Wang, Lei},
  journal={IEEE Access}, 
  title={A Novel Document-Level Relation Extraction Method Based on BERT and Entity Information}, 
  year={2020},
  volume={8},
  number={},
  pages={96912-96919},
  keywords={Feature extraction;Bit error rate;Data mining;Task analysis;Semantics;Australia;Neural networks;Relation extraction;document-level;BERT;entity information;one-pass},
  doi={10.1109/ACCESS.2020.2996642}
}
@inproceedings{huang-etal-2021-three,
    title = "Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction",
    author = "Huang, Quzhe  and
      Zhu, Shengqi  and
      Feng, Yansong  and
      Ye, Yuan  and
      Lai, Yuxuan  and
      Zhao, Dongyan",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.126",
    doi = "10.18653/v1/2021.acl-short.126",
    pages = "998--1004",
    abstract = "Document-level Relation Extraction (RE) is a more challenging task than sentence RE as it often requires reasoning over multiple sentences. Yet, human annotators usually use a small number of sentences to identify the relationship between a given entity pair. In this paper, we present an embarrassingly simple but effective method to heuristically select evidence sentences for document-level RE, which can be easily combined with BiLSTM to achieve good performance on benchmark datasets, even better than fancy graph neural network based methods. We have released our code at \url{https://github.com/AndrewZhe/Three-Sentences-Are-All-You-Need}."
}

@inproceedings{zhang-etal-2020-document,
    title = "Document-level Relation Extraction with Dual-tier Heterogeneous Graph",
    author = "Zhang, Zhenyu  and
      Yu, Bowen  and
      Shu, Xiaobo  and
      Liu, Tingwen  and
      Tang, Hengzhu  and
      Yubin, Wang  and
      Guo, Li",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.143",
    doi = "10.18653/v1/2020.coling-main.143",
    pages = "1630--1641"
}
@article{10.1093/database/bax024,
    author = {Gu, Jinghang and Sun, Fuqing and Qian, Longhua and Zhou, Guodong},
    title = "{Chemical-induced disease relation extraction via convolutional neural network}",
    journal = {Database},
    volume = {2017},
    pages = {bax024},
    year = {2017},
    month = {04},
    abstract = "{This article describes our work on the BioCreative-V chemical–disease relation (CDR) extraction task, which employed a maximum entropy (ME) model and a convolutional neural network model for relation extraction at inter- and intra-sentence level, respectively. In our work, relation extraction between entity concepts in documents was simplified to relation extraction between entity mentions. We first constructed pairs of chemical and disease mentions as relation instances for training and testing stages, then we trained and applied the ME model and the convolutional neural network model for inter- and intra-sentence level, respectively. Finally, we merged the classification results from mention level to document level to acquire the final relations between chemical and disease concepts. The evaluation on the BioCreative-V CDR corpus shows the effectiveness of our proposed approach.Database URL:http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/}",
    issn = {1758-0463},
    doi = {10.1093/database/bax024},
    url = {https://doi.org/10.1093/database/bax024},
    eprint = {https://academic.oup.com/database/article-pdf/doi/10.1093/database/bax024/19233027/bax024.pdf},
}

@article{li2018chemical,
  title={Chemical-induced disease extraction via recurrent piecewise convolutional neural networks},
  author={Li, Haodi and Yang, Ming and Chen, Qingcai and Tang, Buzhou and Wang, Xiaolong and Yan, Jun},
  journal={BMC medical informatics and decision making},
  volume={18},
  pages={45--51},
  year={2018},
  publisher={Springer}
}
@article{mandya2018combining,
title={Combining Long Short Term Memory and Convolutional Neural Network for Cross-Sentence n-ary Relation Extraction},
author={Angrosh Mandya and Danushka Bollegala and Frans Coenen and Katie Atkinson},
booktitle={Automated Knowledge Base Construction (AKBC)},
year={2019},
url={https://openreview.net/forum?id=Sye0lZqp6Q}
}

@inproceedings{wu2019renet,
  title={Renet: A deep learning approach for extracting gene-disease associations from literature},
  author={Wu, Ye and Luo, Ruibang and Leung, Henry CM and Ting, Hing-Fung and Lam, Tak-Wah},
  booktitle={Research in Computational Molecular Biology: 23rd Annual International Conference, RECOMB 2019, Washington, DC, USA, May 5-8, 2019, Proceedings 23},
  pages={272--284},
  year={2019},
  organization={Springer}
}
@article{renet2,
    author = {Su, Junhao and Wu, Ye and Ting, Hing-Fung and Lam, Tak-Wah and Luo, Ruibang},
    title = "{RENET2: high-performance full-text gene–disease relation extraction with iterative training data expansion}",
    journal = {NAR Genomics and Bioinformatics},
    volume = {3},
    number = {3},
    pages = {062},
    year = {2021},
    month = {07},
    abstract = "{Relation extraction (RE) is a fundamental task for extracting gene–disease associations from biomedical text. Many state-of-the-art tools have limited capacity, as they can extract gene–disease associations only from single sentences or abstract texts. A few studies have explored extracting gene–disease associations from full-text articles, but there exists a large room for improvements. In this work, we propose RENET2, a deep learning-based RE method, which implements Section Filtering and ambiguous relations modeling to extract gene–disease associations from full-text articles. We designed a novel iterative training data expansion strategy to build an annotated full-text dataset to resolve the scarcity of labels on full-text articles. In our experiments, RENET2 achieved an F1-score of 72.13\\% for extracting gene–disease associations from an annotated full-text dataset, which was 27.22, 30.30, 29.24 and 23.87\\% higher than BeFree, DTMiner, BioBERT and RENET, respectively. We applied RENET2 to (i) ∼1.89M full-text articles from PubMed Central and found ∼3.72M gene–disease associations; and (ii) the LitCovid articles and ranked the top 15 proteins associated with COVID-19, supported by recent articles. RENET2 is an efficient and accurate method for full-text gene–disease association extraction. The source-code, manually curated abstract/full-text training data, and results of RENET2 are available at GitHub.}",
    issn = {2631-9268},
    doi = {10.1093/nargab/lqab062},
    url = {https://doi.org/10.1093/nargab/lqab062},
    eprint = {https://academic.oup.com/nargab/article-pdf/3/3/lqab062/56833744/lqab062.pdf},
}

@inproceedings{verga-etal-2018-simultaneously,
    title = "Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction",
    author = "Verga, Patrick  and
      Strubell, Emma  and
      McCallum, Andrew",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1080",
    doi = "10.18653/v1/N18-1080",
    pages = "872--884",
    abstract = "Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.",
}

@article{corr,
  author       = {Hong Wang and
                  Christfried Focke and
                  Rob Sylvester and
                  Nilesh Mishra and
                  William Yang Wang},
  title        = {Fine-tune Bert for DocRED with Two-step Process},
  journal      = {CoRR},
  volume       = {abs/1909.11898},
  year         = {2019},
  url          = {http://arxiv.org/abs/1909.11898},
  eprinttype    = {arXiv},
  eprint       = {1909.11898},
  timestamp    = {Tue, 16 May 2023 16:54:39 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1909-11898.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NIPS2013_1cecc7a7,
 author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {2787–2795},
 publisher = {Curran Associates, Inc.},
 title = {Translating Embeddings for Modeling Multi-relational Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf},
 volume = {26},
 year = {2013}
}


@Article{app12031599,
AUTHOR = {Kuang, Hailan and Chen, Haoran and Ma, Xiaolin and Liu, Xinhua},
TITLE = {A Keyword Detection and Context Filtering Method for Document Level Relation Extraction},
JOURNAL = {Applied Sciences},
VOLUME = {12},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {1599},
URL = {https://www.mdpi.com/2076-3417/12/3/1599},
pages = {2076-3417},
ABSTRACT = {Relation extraction (RE) is the core link of downstream tasks, such as information retrieval, question answering systems, and knowledge graphs. Most of the current mainstream RE technologies focus on the sentence-level corpus, which has great limitations in practical applications. Moreover, the previously proposed models based on graph neural networks or transformers try to obtain context features from the global text, ignoring the importance of local features. In practice, the relation between entity pairs can usually be inferred just through a few keywords. This paper proposes a keyword detection and context filtering method based on the Self-Attention mechanism for document-level RE. In addition, a Self-Attention Memory (SAM) module in ConvLSTM is introduced to process the document context and capture keyword features. By searching for word embeddings with high cross-attention of entity pairs, we update and record critical local features to enhance the performance of the final classification model. The experimental results on three benchmark datasets (DocRED, CDR, and GBA) show that our model achieves advanced performance within open and specialized domain relationship extraction tasks, with up to 0.87% F1 value improvement compared to the state-of-the-art methods. We have also designed experiments to demonstrate that our model can achieve superior results by its stronger contextual filtering capability compared to other methods.},
DOI = {10.3390/app12031599}
}

@inproceedings{yu-etal-2022-relation,
    title = "Relation-Specific Attentions over Entity Mentions for Enhanced Document-Level Relation Extraction",
    author = "Yu, Jiaxin  and
      Yang, Deqing  and
      Tian, Shuyu",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.109",
    doi = "10.18653/v1/2022.naacl-main.109",
    pages = "1523--1529",
    abstract = "Compared with traditional sentence-level relation extraction, document-level relation extraction is a more challenging task where an entity in a document may be mentioned multiple times and associated with multiple relations. However, most methods of document-level relation extraction do not distinguish between mention-level features and entity-level features, and just apply simple pooling operation for aggregating mention-level features into entity-level features. As a result, the distinct semantics between the different mentions of an entity are overlooked. To address this problem, we propose RSMAN in this paper which performs selective attentions over different entity mentions with respect to candidate relations. In this manner, the flexible and relation-specific representations of entities are obtained which indeed benefit relation classification. Our extensive experiments upon two benchmark datasets show that our RSMAN can bring significant improvements for some backbone models to achieve state-of-the-art performance, especially when an entity have multiple mentions in the document.",
}

@inproceedings{xiao-etal-2020-denoising,
    title = "Denoising Relation Extraction from Document-level Distant Supervision",
    author = "Xiao, Chaojun  and
      Yao, Yuan  and
      Xie, Ruobing  and
      Han, Xu  and
      Liu, Zhiyuan  and
      Sun, Maosong  and
      Lin, Fen  and
      Lin, Leyu",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.300",
    doi = "10.18653/v1/2020.emnlp-main.300",
    pages = "3683--3688",
    abstract = "Distant supervision (DS) has been widely adopted to generate auto-labeled data for sentence-level relation extraction (RE) and achieved great results. However, the existing success of DS cannot be directly transferred to more challenging document-level relation extraction (DocRE), as the inevitable noise caused by DS may be even multiplied in documents and significantly harm the performance of RE. To alleviate this issue, we propose a novel pre-trained model for DocRE, which de-emphasize noisy DS data via multiple pre-training tasks. The experimental results on the large-scale DocRE benchmark show that our model can capture useful information from noisy data and achieve promising results.",
}

@inproceedings{tan-etal-2022-document,
    title = "Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation",
    author = "Tan, Qingyu  and
      He, Ruidan  and
      Bing, Lidong  and
      Ng, Hwee Tou",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.132",
    doi = "10.18653/v1/2022.findings-acl.132",
    pages = "1672--1681",
    abstract = "Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entity-pairs, which improves the performance on two-hop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign{\_}F1 score on the DocRED leaderboard.",
}

@article{Xu_Wang_Lyu_Zhu_Mao_2021, title={Entity Structure Within and Throughout: Modeling Mention Dependencies for Document-Level Relation Extraction}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17665}, abstractNote={Entities, as the essential elements in relation extraction tasks, exhibit certain structure. In this work, we formulate such entity structure as distinctive dependencies between mention pairs. We then propose SSAN, which incorporates these structural dependencies within the standard self-attention mechanism and throughout the overall encoding stage. Specifically, we design two alternative transformation modules inside each self-attention building block to produce attentive biases so as to adaptively regularize its attention flow. Our experiments demonstrate the usefulness of the proposed entity structure and the effectiveness of SSAN. It significantly outperforms competitive baselines, achieving new state-of-the-art results on three popular document-level relation extraction datasets. We further provide ablation and visualization to show how the entity structure guides the model for better relation extraction. Our code is publicly available.}, number={16}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Xu, Benfeng and Wang, Quan and Lyu, Yajuan and Zhu, Yong and Mao, Zhendong}, year={2021}, month={May}, pages={14149-14157} }

@article{Vaswani2017AttentionIA,
  added-at = {2020-01-09T12:02:01.000+0100},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  biburl = {https://www.bibsonomy.org/bibtex/26c1129dc79583078fb0409fc7efda6aa/annakrause},
  description = {Aktuelleres Paper zur Verwendung von Attention für die Neural Machine Translation},
  ee = {http://arxiv.org/abs/1706.03762},
  interhash = {b23d83da70543e00f9240cc009f1fcfa},
  intrahash = {6c1129dc79583078fb0409fc7efda6aa},
  journal = {CoRR},
  keywords = {Attention Transformer machinelearning},
  note = {cite arxiv:1706.03762Comment: 15 pages, 5 figures},
  timestamp = {2020-01-09T12:02:01.000+0100},
  title = {Attention Is All You Need},
  url = {http://arxiv.org/abs/1706.03762},
  volume = {abs/1706.03762},
  year = 2017
}

@inproceedings{quirk-poon-2017-distant,
    title = "Distant Supervision for Relation Extraction beyond the Sentence Boundary",
    author = "Quirk, Chris  and
      Poon, Hoifung",
    editor = "Lapata, Mirella  and
      Blunsom, Phil  and
      Koller, Alexander",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-1110",
    pages = "1171--1182",
    abstract = "The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However, existing distance supervision approaches only extract relations expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this paper, we propose the first approach for applying distant supervision to cross-sentence relation extraction. At the core of our approach is a graph representation that can incorporate both standard dependencies and discourse relations, thus providing a unifying way to model relations within and across sentences. We extract features from multiple paths in this graph, increasing accuracy and robustness when confronted with linguistic variation and analysis error. Experiments on an important extraction task for precision medicine show that our approach can learn an accurate cross-sentence extractor, using only a small existing knowledge base and unlabeled text from biomedical research articles. Compared to the existing distant supervision paradigm, our approach extracted twice as many relations at similar precision, thus demonstrating the prevalence of cross-sentence relations and the promise of our approach.",
}

@article{peng-etal-2017-cross,
    title = "Cross-Sentence N-ary Relation Extraction with Graph {LSTM}s",
    author = "Peng, Nanyun  and
      Poon, Hoifung  and
      Quirk, Chris  and
      Toutanova, Kristina  and
      Yih, Wen-tau",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q17-1008",
    doi = "10.1162/tacl_a_00049",
    pages = "101--115",
    abstract = "Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and inter-sentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier. This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.",
}

@inproceedings{tai-etal-2015-improved,
    title = "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
    author = "Tai, Kai Sheng  and
      Socher, Richard  and
      Manning, Christopher D.",
    editor = "Zong, Chengqing  and
      Strube, Michael",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-1150",
    doi = "10.3115/v1/P15-1150",
    pages = "1556--1566",
}

@inproceedings{christopoulou-etal-2018-walk,
    title = "A Walk-based Model on Entity Graphs for Relation Extraction",
    author = "Christopoulou, Fenia  and
      Miwa, Makoto  and
      Ananiadou, Sophia",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-2014",
    doi = "10.18653/v1/P18-2014",
    pages = "81--88",
    abstract = "We present a novel graph-based neural network model for relation extraction. Our model treats multiple pairs in a sentence simultaneously and considers interactions among them. All the entities in a sentence are placed as nodes in a fully-connected graph structure. The edges are represented with position-aware contexts around the entity pairs. In order to consider different relation paths between two entities, we construct up to $l$-length walks between each pair. The resulting walks are merged and iteratively used to update the edge representations into longer walks representations. We show that the model achieves performance comparable to the state-of-the-art systems on the ACE 2005 dataset without using any external tools.",
}

@inproceedings{christopoulou-etal-2019-connecting,
    title = "Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs",
    author = "Christopoulou, Fenia  and
      Miwa, Makoto  and
      Ananiadou, Sophia",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1498",
    doi = "10.18653/v1/D19-1498",
    pages = "4925--4936",
    abstract = "Document-level relation extraction is a complex human process that requires logical inference to extract relationships between named entities in text. Existing approaches use graph-based neural models with words as nodes and edges as relations between them, to encode relations across sentences. These models are node-based, i.e., they form pair representations based solely on the two target node representations. However, entity relations can be better expressed through unique edge representations formed as paths between nodes. We thus propose an edge-oriented graph neural model for document-level relation extraction. The model utilises different types of nodes and edges to create a document-level graph. An inference mechanism on the graph edges enables to learn intra- and inter-sentence relations using multi-instance learning internally. Experiments on two document-level biomedical datasets for chemical-disease and gene-disease associations show the usefulness of the proposed edge-oriented approach.",
}

@inproceedings{sahu-etal-2019-inter,
    title = "Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network",
    author = "Sahu, Sunil Kumar  and
      Christopoulou, Fenia  and
      Miwa, Makoto  and
      Ananiadou, Sophia",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1423",
    doi = "10.18653/v1/P19-1423",
    pages = "4309--4316",
    abstract = "Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.",
}

@inproceedings{guo-etal-2019-attention,
    title = "Attention Guided Graph Convolutional Networks for Relation Extraction",
    author = "Guo, Zhijiang  and
      Zhang, Yan  and
      Lu, Wei",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1024",
    doi = "10.18653/v1/P19-1024",
    pages = "241--251",
    abstract = "Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text. However, how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question. Existing approaches employing rule based hard-pruning strategies for selecting relevant partial dependency structures may not always yield optimal results. In this work, we propose Attention Guided Graph Convolutional Networks (AGGCNs), a novel model which directly takes full dependency trees as inputs. Our model can be understood as a soft-pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task. Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees, giving significantly better results than previous approaches.",
}

@article{Gupta_Rajaram_Schütze_Runkler_2019, title={Neural Relation Extraction within and across Sentence Boundaries}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4617}, DOI={10.1609/aaai.v33i01.33016513}, abstractNote={&lt;p&gt;Past work in relation extraction mostly focuses on binary relation between entity pairs &lt;em&gt;within single sentence&lt;/em&gt;. Recently, the NLP community has gained interest in relation extraction in entity pairs &lt;em&gt;spanning multiple sentences&lt;/em&gt;. In this paper, we propose a novel architecture for this task: inter-sentential dependency-based neural networks (iDepNN). iDepNN models the shortest and augmented dependency paths via recurrent and recursive neural networks to extract relationships within (intra-) and across (inter-) sentence boundaries. Compared to SVM and neural network baselines, iDepNN is more robust to false positives in relationships spanning sentences. We evaluate our models on four datasets from newswire (MUC6) and medical (BioNLP shared task) domains that achieve state-of-the-art performance and show a better balance in precision and recall for inter-sentential relationships. We perform better than 11 teams participating in the BioNLP shared task 2016 and achieve a gain of 5.2% (0.587 vs 0.558) in &lt;em&gt;F&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt; over the winning team. We also release the crosssentence annotations for MUC6.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Gupta, Pankaj and Rajaram, Subburam and Schütze, Hinrich and Runkler, Thomas}, year={2019}, month={Jul.}, pages={6513-6520} }

@inproceedings{minh-tran-etal-2020-dots,
    title = "The Dots Have Their Values: Exploiting the Node-Edge Connections in Graph-based Neural Models for Document-level Relation Extraction",
    author = "Minh Tran, Hieu  and
      Nguyen, Minh Trung  and
      Nguyen, Thien Huu",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.409",
    doi = "10.18653/v1/2020.findings-emnlp.409",
    pages = "4561--4567",
    abstract = "The goal of Document-level Relation Extraction (DRE) is to recognize the relations between entity mentions that can span beyond sentence boundary. The current state-of-the-art method for this problem has involved the graph-based edge-oriented model where the entity mentions, entities, and sentences in the documents are used as the nodes of the document graphs for representation learning. However, this model does not capture the representations for the nodes in the graphs, thus preventing it from effectively encoding the specific and relevant information of the nodes for DRE. To address this issue, we propose to explicitly compute the representations for the nodes in the graph-based edge-oriented model for DRE. These node representations allow us to introduce two novel representation regularization mechanisms to improve the representation vectors for DRE. The experiments show that our model achieves state-of-the-art performance on two benchmark datasets.",
}

@inproceedings{luan-etal-2019-general,
    title = "A general framework for information extraction using dynamic span graphs",
    author = "Luan, Yi  and
      Wadden, Dave  and
      He, Luheng  and
      Shah, Amy  and
      Ostendorf, Mari  and
      Hajishirzi, Hannaneh",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1308",
    doi = "10.18653/v1/N19-1308",
    pages = "3036--3046",
    abstract = "We introduce a general framework for several information extraction tasks that share span representations using dynamically constructed span graphs. The graphs are dynamically constructed by selecting the most confident entity spans and linking these nodes with confidence-weighted relation types and coreferences. The dynamic span graph allow coreference and relation type confidences to propagate through the graph to iteratively refine the span representations. This is unlike previous multi-task frameworks for information extraction in which the only interaction between tasks is in the shared first-layer LSTM. Our framework significantly outperforms state-of-the-art on multiple information extraction tasks across multiple datasets reflecting different domains. We further observe that the span enumeration approach is good at detecting nested span entities, with significant F1 score improvement on the ACE dataset.",
}

@article{WANG2021107274,
title = {Document-level relation extraction using evidence reasoning on RST-GRAPH},
journal = {Knowledge-Based Systems},
volume = {228},
pages = {107274},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107274},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121005360},
author = {Hailin Wang and Ke Qin and Guoming Lu and Jin Yin and Rufai Yusuf Zakari and Jim Wilson Owusu},
keywords = {Relation extraction, Rhetorical structure theory, Document-level, Evidence reasoning},
abstract = {Document-level relation extraction (RE) is a more challenging task providing a new perspective to solve larger and more complex text mining work. Recent document-level RE research following traditional sentence-level methods focus on learning a character representation of one sentence, highlighting the importance of partial words. When applied to a document with a longer text, more entities and more complicated semantics, these traditional methods lacking the ability to select evidence and further reason on them, may not be sufficient to identify all potential relation in the document in an intuitive way. However, some textual semantic associations between entities as well as logical structure of document could provide evidence to the potential relation and explain the reasoning process. Hence, by introducing Rhetorical Structure Theory (RST) as an external knowledge, this article attempts to select appropriate evidence and show reasoning process on a new document-graph, RST-GRAPH, which indicators valid semantic associations between multiple text units through RST and incorporates a set of reasoning modules to capture efficient evidence. Our experimental result shows that the RST-GRAPH, as the first work introducing RST, builds associations in an interpretable manner along discourse relation link between any entities compared to previous graph-oriented models, exhibits a clear process of evidence reasoning with competitive performance on DocRED dataset, and outperforms most existing models on the value of Ign F1.}
}

@article{LI2022109146,
title = {Heterogenous affinity graph inference network for document-level relation extraction},
journal = {Knowledge-Based Systems},
volume = {250},
pages = {109146},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109146},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122005706},
author = {Rongzhen Li and Jiang Zhong and Zhongxuan Xue and Qizhu Dai and Xue Li},
keywords = {Document-level relation extraction, Graph convolutional network, Relation reasoning},
abstract = {Document-level relation extraction (Doc-level RE) is a more practical and challenging task, which provides a new perspective on obtaining factual knowledge from the more complex cross-sentence text. Recent Doc-level RE, based on pre-trained language models, uses graph neural networks to implicitly model relation reasoning in a document. However, it is not perfect that the model neglects explicit reasoning clues, leading to a weak ability and a lack of capability to model long-distance relationships. In this paper, we propose to explicitly model the heterogeneous affinity graph, HAG, including a mention graph (MG) and a coreference graph (CG). We first construct CG to cluster the expressions together as a coreference array. Then, MG and CG are incorporated to capture the reasoning clues from the adjacent affinity matrix. Moreover, HAG is aggregated into an isomorphic entity graph according to the noise suppression mechanism and RGCN. Finally, the classification is established on the normalized graph to infer the relations of entity pairs. Experimental results significantly outperform baselines by nearly 1.7% ∼ 2.0% in F1 on three public datasets, DocRED, DialogRE, and MPDD. We further conduct ablation experiments to demonstrate the effectiveness of the proposed approach.}
}


@article{barreyro2012working,
  title={Working memory capacity and individual differences in the making of reinstatement and elaborative inferences},
  author={Barreyro, Juan Pablo and Cevasco, Jazm{\'\i}n and Bur{\'\i}n, D{\'e}bora and Marotto, Carlos Molinari},
  journal={The Spanish journal of psychology},
  volume={15},
  number={2},
  pages={471--479},
  year={2012},
  publisher={Cambridge University Press}
}

@inproceedings{li-etal-2021-mrn,
    title = "{MRN}: A Locally and Globally Mention-Based Reasoning Network for Document-Level Relation Extraction",
    author = "Li, Jingye  and
      Xu, Kang  and
      Li, Fei  and
      Fei, Hao  and
      Ren, Yafeng  and
      Ji, Donghong",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.117",
    doi = "10.18653/v1/2021.findings-acl.117",
    pages = "1359--1370",
}


@article{app10031181,
  title={GREG: A global level relation extraction with knowledge graph embedding},
  author={Kim, Kuekyeng and Hur, Yuna and Kim, Gyeongmin and Lim, Heuiseok},
  journal={Applied Sciences},
  volume={10},
  number={3},
  pages={1181},
  year={2020},
  publisher={MDPI}
}

@inproceedings{zeng-etal-2021-sire,
    title = "{SIRE}: Separate Intra- and Inter-sentential Reasoning for Document-level Relation Extraction",
    author = "Zeng, Shuang  and
      Wu, Yuting  and
      Chang, Baobao",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.47",
    doi = "10.18653/v1/2021.findings-acl.47",
    pages = "524--534",
}

